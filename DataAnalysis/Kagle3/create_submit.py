#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è submit.csv
–ó–∞–ø—É—Å–∫: python create_submit.py
"""

import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from qa_model import predict_with_gigachat_improved, get_default_prediction_by_category
import pandas as pd

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è submit.csv"""
    
    print("="*70)
    print("–ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤")
    print("="*70 + "\n")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞
    token_file = 'authGigaChat.txt'
    if not os.path.exists(token_file):
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {token_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print(f"–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª {token_file} —Å —Ç–æ–∫–µ–Ω–æ–º GigaChat API")
        return
    
    try:
        token = open(token_file).read().strip()
        if not token:
            print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {token_file} –ø—É—Å—Ç!")
            return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {token_file}: {e}")
        return
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–∞–Ω–Ω—ã—Ö
    train_path = '/Users/aleksey/Downloads/hw-3-questions-and-answering/train.csv'
    test_path = '/Users/aleksey/Downloads/hw-3-questions-and-answering/test.csv'
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(train_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {train_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    if not os.path.exists(test_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {test_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    try:
        df_train = pd.read_csv(train_path)
        df_test = pd.read_csv(test_path)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–æ–≤: {e}")
        return
    
    print(f"‚úì –û–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä: {len(df_train)} –∑–∞–ø–∏—Å–µ–π")
    print(f"‚úì –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {len(df_test)} –∑–∞–ø–∏—Å–µ–π")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–±—É—á–∞—é—â–µ–º—É –Ω–∞–±–æ—Ä—É
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞:")
    answer_dist = df_train['–æ—Ç–≤–µ—Ç'].value_counts().sort_index()
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤:")
    for ans, count in answer_dist.items():
        print(f"     {ans}: {count} ({100*count/len(df_train):.1f}%)")
    most_common = df_train['–æ—Ç–≤–µ—Ç'].mode()[0]
    print(f"   –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π –æ—Ç–≤–µ—Ç: {most_common}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    print("\n" + "="*70)
    print("üöÄ –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞...")
    print("="*70 + "\n")
    
    try:
        df_results = predict_with_gigachat_improved(
            df_test,
            token=token,
            df_train=df_train,
            max_retries=3,
            delay_between_requests=0.1,
            timeout=60,
            batch_size=50,
            delay_between_batches=2.0,
            use_few_shot=True
        )
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º!")
        print("–ß–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã...")
        return
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∑–∞–º–µ–Ω—è–µ–º -1 –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    failed_count = (df_results['prediction'] == -1).sum()
    if failed_count > 0:
        print(f"\nüîß –ó–∞–º–µ–Ω–∞ {failed_count} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        for idx, row in df_results[df_results['prediction'] == -1].iterrows():
            test_row = df_test[df_test['id'] == row['id']]
            category = test_row['–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].values[0] if len(test_row) > 0 else None
            df_results.loc[idx, 'prediction'] = get_default_prediction_by_category(df_train, category)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*70)
    print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    print("="*70)
    print(f"   –í—Å–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(df_results)}")
    valid_predictions = df_results['prediction'].between(0, 3)
    print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (0-3): {valid_predictions.sum()}")
    print(f"   –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {(~valid_predictions).sum()}")
    
    print("\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    pred_dist = df_results['prediction'].value_counts().sort_index()
    for pred, count in pred_dist.items():
        print(f"     {pred}: {count} ({100*count/len(df_results):.1f}%)")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = 'submit.csv'
    df_results.to_csv(output_file, index=False)
    
    print("\n" + "="*70)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    print("="*70)
    print(f"   –§–æ—Ä–º–∞—Ç: id, prediction")
    print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(output_file)} –±–∞–π—Ç")
    print(f"   –ì–æ—Ç–æ–≤–æ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ!")
    print("="*70)

if __name__ == "__main__":
    main()

